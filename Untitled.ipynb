{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba97b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#sentimental analysis\n",
    "\n",
    "#importing the needed lib\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "\n",
    "\n",
    "# Global Parameters\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d571164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting variables to be used below\n",
    "maxTweets = 10000\n",
    "\n",
    "# Creating list to append tweet data to\n",
    "tweets_list2 = []\n",
    "\n",
    "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('since #ASUUstrike,#ASUU:2022-02-15 until:2022-07-20').get_items()):\n",
    "    if i>maxTweets:\n",
    "        break\n",
    "    tweets_list2.append([tweet.date, tweet.id, tweet.content, tweet.user.username])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca64ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our search term, using syntax for Twitter's Advanced Search\n",
    "search = '\"ASUU\",\"ASUUstrike\",\"ASUUstrikeupdate\"'\n",
    "\n",
    "# the scraped tweets, this is a generator\n",
    "scraped_tweets = sntwitter.TwitterSearchScraper(search).get_items()\n",
    "\n",
    "# slicing the generator to keep only the first 2000 tweets\n",
    "sliced_scraped_tweets = itertools.islice(scraped_tweets, 10000)\n",
    "\n",
    "# convert to a DataFrame and keep only relevant columns\n",
    "df = pd.DataFrame(sliced_scraped_tweets)[['date', 'content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame(itertools.islice(sntwitter.TwitterSearchScraper(\n",
    "    '\"ASUU\",\"ASUUstrike\",\"ASUUstrikeupdate\"').get_items(), 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv('asuu_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d57d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting data info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ba4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d71b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856df2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dad588",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb55d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd8c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.drop(['url', 'content', 'renderedContent', 'id','outlinks', 'tcooutlinks',\n",
    "       'media'],inplace= True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e477edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0517a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.drop(['retweetedTweet', 'quotedTweet', 'inReplyToTweetId', 'inReplyToUser',\n",
    "       'mentionedUsers','place', 'conversationId', 'lang', 'source', 'sourceUrl','coordinates','cashtags' ],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e845da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe5d390",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.loc[:,'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e2abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from datetime import datetime, timedelta\n",
    "#futuredate = datetime.now() + timedelta(days=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648183f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#date = datetime(int(year), int(month), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd706388",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting time to st\n",
    "tweets['date'] = pd.to_datetime(tweets['date'],unit='ms').dt.tz_convert('Europe/Vatican')\n",
    "tweets['date'] = tweets['date'].apply(lambda a: datetime.datetime.strftime(a,\"%Y-%m-%d %H:%M:%S\"))\n",
    "tweets['date'] = pd.to_datetime(tweets['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d59e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.loc[:,'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.loc[:,'hashtags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bed8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.hashtags.str.split(\",\",expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a9e2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_remove = ['.', '-', '(', ')', '']\n",
    "regular_expression = '[' + re.escape (''. join (chars_to_remove)) + ']'\n",
    "\n",
    "df['string_col'].str.replace(regular_expression, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454712c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_excel('final_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0516f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16517e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c32b905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet_text(tweet):\n",
    "    tweet.lower()\n",
    "    # Remove urls\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
    "    # Remove user @ references and '#' from tweet\n",
    "    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
    "    # Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
    "    \n",
    "    #ps = PorterStemmer()\n",
    "    #stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n",
    "    \n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724986e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector(train_fit):\n",
    "    vector = TfidfVectorizer(sublinear_tf=True)\n",
    "    vector.fit(train_fit)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8597375d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_string(sentiment):\n",
    "    if sentiment == 0:\n",
    "        return \"Negative\"\n",
    "    elif sentiment == 2:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baf2859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for sentiments\n",
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "data = [\"tweets\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50221539",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_model = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "specific_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ca75e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db26035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e3809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d0f2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d121bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864862c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into Train, Test\n",
    "\n",
    "# Same tf vector will be used for Testing sentiments on unseen trending data\n",
    "tf_vector = get_feature_vector(np.array(dataset.iloc[:, 1]).ravel())\n",
    "X = tf_vector.transform(np.array(dataset.iloc[:, 1]).ravel())\n",
    "y = np.array(dataset.iloc[:, 0]).ravel()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6, random_state=40)\n",
    "\n",
    "# Training Naive Bayes model\n",
    "NB_model = MultinomialNB()\n",
    "NB_model.fit(X_train, y_train)\n",
    "y_predict_nb = NB_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_predict_nb))\n",
    "\n",
    "# Training Logistics Regression model\n",
    "LR_model = LogisticRegression(solver='lbfgs')\n",
    "LR_model.fit(X_train, y_train)\n",
    "y_predict_lr = LR_model.predict(X_test)\n",
    "print(accuracy_score(y_test, y_predict_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe38712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a245d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet_text(tweet):\n",
    "    tweet.lower()\n",
    "    # Remove urls\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
    "    # Remove user @ references and '#' from tweet\n",
    "    tweet = re.sub(r'\\@\\w+|\\#','', tweet)\n",
    "    # Remove punctuations\n",
    "    tweet = tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Remove stopwords\n",
    "    tweet_tokens = word_tokenize(tweet)\n",
    "    filtered_words = [w for w in tweet_tokens if not w in stop_words]\n",
    "    \n",
    "    #ps = PorterStemmer()\n",
    "    #stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #lemma_words = [lemmatizer.lemmatize(w, pos='a') for w in stemmed_words]\n",
    "    \n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fd6d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectoring\n",
    "def get_feature_vector(train_fit):\n",
    "    vector = TfidfVectorizer(sublinear_tf=True)\n",
    "    vector.fit(train_fit)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4154c736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_string(sentiment):\n",
    "    if sentiment == 0:\n",
    "        return \"Negative\"\n",
    "    elif sentiment == 2:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return \"Positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea828ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5e66d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for sentiments\n",
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "data = [\"tweets\"]\n",
    "sentiment_pipeline(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_model = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")\n",
    "specific_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85338464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06752e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95748d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
